{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0faa6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on tutorial: https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7096afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch includes many domain specific libraries (TorchText, TorchVision, TorchAudio...) that include datasets. Here, we use a TorchVision dataset.\n",
    "Each Dataset includes two arguments to modify the samples and the labels respectively (transform and target_transform).\n",
    "\"\"\"\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a3cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nQuestions: How do I get data that I upload into a DataLoader?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "By wrapping the PyTorch DataLoader around the Dataset, we create an iterable. This iterable (DataLoader) supports automatic batching, sampling, shuffling, and multiprocess data loading, and thus is very powerful.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\"\"\" \n",
    "Questions: How do I get data that I upload into a DataLoader?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1ab796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In PyTorch, we build neural networks by creating a class that inherits from nn.Module. The layers of the network\n",
    "are defined in the __init__() function, and we define how data flows through the network in the forward() function. An\n",
    "accelerator, such as CUDA, MPS, MTIA, or XPU, can be added to speed up variuos aspects of training. If this is not \n",
    "available, PyTorch will default to the cpu.\n",
    "\"\"\"\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabe54b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304651  [   64/60000]\n",
      "loss: 2.292750  [ 6464/60000]\n",
      "loss: 2.261925  [12864/60000]\n",
      "loss: 2.267838  [19264/60000]\n",
      "loss: 2.254951  [25664/60000]\n",
      "loss: 2.209020  [32064/60000]\n",
      "loss: 2.237845  [38464/60000]\n",
      "loss: 2.190547  [44864/60000]\n",
      "loss: 2.184268  [51264/60000]\n",
      "loss: 2.157164  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 2.149420 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.164298  [   64/60000]\n",
      "loss: 2.152345  [ 6464/60000]\n",
      "loss: 2.084271  [12864/60000]\n",
      "loss: 2.112251  [19264/60000]\n",
      "loss: 2.066629  [25664/60000]\n",
      "loss: 1.994694  [32064/60000]\n",
      "loss: 2.036831  [38464/60000]\n",
      "loss: 1.948733  [44864/60000]\n",
      "loss: 1.949462  [51264/60000]\n",
      "loss: 1.883520  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.883447 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.915007  [   64/60000]\n",
      "loss: 1.888554  [ 6464/60000]\n",
      "loss: 1.762947  [12864/60000]\n",
      "loss: 1.817472  [19264/60000]\n",
      "loss: 1.713506  [25664/60000]\n",
      "loss: 1.654388  [32064/60000]\n",
      "loss: 1.679840  [38464/60000]\n",
      "loss: 1.578405  [44864/60000]\n",
      "loss: 1.599983  [51264/60000]\n",
      "loss: 1.498484  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.520778 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.584738  [   64/60000]\n",
      "loss: 1.554806  [ 6464/60000]\n",
      "loss: 1.398614  [12864/60000]\n",
      "loss: 1.481653  [19264/60000]\n",
      "loss: 1.363579  [25664/60000]\n",
      "loss: 1.356160  [32064/60000]\n",
      "loss: 1.365045  [38464/60000]\n",
      "loss: 1.291913  [44864/60000]\n",
      "loss: 1.325536  [51264/60000]\n",
      "loss: 1.229591  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.257514 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.333922  [   64/60000]\n",
      "loss: 1.319392  [ 6464/60000]\n",
      "loss: 1.147628  [12864/60000]\n",
      "loss: 1.261174  [19264/60000]\n",
      "loss: 1.133985  [25664/60000]\n",
      "loss: 1.161744  [32064/60000]\n",
      "loss: 1.173238  [38464/60000]\n",
      "loss: 1.114616  [44864/60000]\n",
      "loss: 1.152470  [51264/60000]\n",
      "loss: 1.074001  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.093472 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhat are the details of the training loop?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To train a model, we must define two things: A loss function and an optimizer.\n",
    "\n",
    "Then we must define the training loop. For each iteration, the model predicts on the training\n",
    "set (split into batches) and backpropogates the error to update the model parameters.\n",
    "\n",
    "Each iteration, we can also check the test loss and accuracy to assess our models learning process.\n",
    "\"\"\"\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "\"\"\"\n",
    "What are the details of the training loop?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce41ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To save a model, we typically save the parameters in a dictionary.\n",
    "When loading a model, we make an instance of it and then load the parameters from the dictionary.\n",
    "\"\"\"\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de39196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we loaded the trained model, we can make predictions with it!\n",
    "\"\"\"\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67331da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Intro to tensors\n",
    "Following turorial: https://docs.pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "Tensors are very similar to ndarrays, but they are optimized to run on GPUs and automatic differentiation. The ndarray and Tensor APIs are very similar, \n",
    "but to review here we go.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f473fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9309, 0.3243],\n",
      "        [0.7573, 0.7000]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0969, 0.0452, 0.4686],\n",
      "        [0.8907, 0.4099, 0.3323]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initializing a tensor\n",
    "\"\"\"\n",
    "\n",
    "# Directly from data\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# From a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "# From another tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "# From random or constant values\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889fe712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attributes of a Tensor\n",
    "\"\"\"\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ae40f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "12.0 <class 'float'>\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Operations on a Tensor\n",
    "\n",
    "There are over 1200 tensor operations supported by PyTorch both on the CPU and on accelerators. Tensors are by default created \n",
    "on the CPU and must be moved to other devices if desired.\n",
    "\"\"\"\n",
    "\n",
    "# We move our tensor to the current accelerator if available\n",
    "if torch.accelerator.is_available():\n",
    "    tensor = tensor.to(torch.accelerator.current_accelerator())\n",
    "\n",
    "# Some operations\n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "# Joining tensors\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "# Arithmetic operations\n",
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "# Single element tensors. If you want a numeric, use .item()\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n",
    "\n",
    "# In-place operations are denoted by a _ suffix to the function name.\n",
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73118a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bridges to NumPy\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share the same memory locations.\n",
    "\"\"\"\n",
    "\n",
    "# Create tensors and convert to NumPy array\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "# Add one to Tensor elements and show NumPy array is affected\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f46e1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building the Neural Network\n",
    "\n",
    "Following Tutorial: https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Nerual networks are composed of layers and modules that operate on data. The torch.nn namespace\n",
    "has everything necessary to build neural networks in PyTorch. Every module in PyTorch subclasses\n",
    "the nn.Module. A network is simply a module that contains other modules, creating an easy to work\n",
    "with hierarchical structure.\n",
    "\"\"\"\n",
    "\n",
    "# Access accelerator if available.\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# Our neural network subclassses the nn.Module.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64d96771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
